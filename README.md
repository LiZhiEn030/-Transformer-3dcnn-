本研究针对传统视频行为识别算法在长时空依赖建模和计算效率方面的不足的问题，提出一种结合3D卷积神经网络（3D-CNN）和Transformer的混合架构。该模型在提升复杂场景识别精度的同时兼顾实时处理需求，通过多尺度时空注意力机制整合动态记忆缓存与优化位置编码方法，使局部特征提取和全局依赖建模模块更好地协同工作。为适应移动端部署，采用MobileNetV3作为骨干网络，并开发了专门设计的双重数据增强策略增强模型泛化能力。
本研究在UCF101数据集上的实验采用动态采样和混合精度训练优化策略。测试数据显示，本研究所提出的算法在准确率、平均精确率、平均召回率及平均 F1 值等关键指标上，分别达到 89.30%、89.80%、88.92% 及 88.80%。在各个指标上相较于传统 3D-CNN 模型和纯 Transformer 模型都有明显的提升。需要指出的是，当前模型在实时场景和小样本条件下的计算效率仍需要改进。对此，后续研究将重点探索稀疏注意力机制和分层网络优化方案，以增强模型的实际应用价值。

通过对四个模型的横向分析比较得知，四个模型的整体上呈现了显著的差距，并且不同参数的模型对不同类别的特征敏感性也不同。在准确度方面，epoch_41模型凭借早停机制避免了过拟合的发生从而以89.30%的准确度居于首位，而epoch_44和Mode_v2都因为过拟合导致准确度下降，v2更是因为比epoch_44多训练了6轮导致准确度只有88.48%，v1模型因为参数batch_size只有v2模型的一半，并且在训练中没有加入早停机制，存在着过拟合的风险，所以v1的准确率只有83.45%。
